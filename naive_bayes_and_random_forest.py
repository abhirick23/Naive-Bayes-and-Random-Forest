# -*- coding: utf-8 -*-
"""Naive_Bayes & Random_Forest

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sEY7mKb_ZoScbDZuIxVBTSPpQNsjI9qW
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive/')
# %cd /gdrive

ls

cd /gdrive/MyDrive/Titanic

ls

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier as rf
import warnings

"""# Uploading Dataset"""

df=pd.read_csv('titanic.csv')
df.head()

df.info()

"""# EDA"""

df.isna().sum()

df.columns

df.Survived.value_counts()

fig, axes = plt.subplots(2, 2, figsize=(12, 7), sharey=True)

sns.countplot("Sex", data=df, ax=axes[0,0])
sns.countplot("Survived", data=df, ax=axes[0,1])
sns.countplot("Pclass", data=df, ax=axes[1,0])
sns.countplot("Embarked", data=df, ax=axes[1,1])

sns.distplot(df["Age"])

df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)
df.head()

input = df.drop('Survived',axis='columns')
target = df.Survived

dummies = pd.get_dummies(input.Sex)
dummies.head()

"""# Creating the input and target states"""

input = pd.concat([input,dummies],axis='columns')
input.head()

input.drop(['Sex','male'],axis='columns',inplace=True)
input.head()

input.Age = input.Age.fillna(input.Age.mean())
input.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(input,target, test_size = 0.25)

"""# Naive Baiyes"""

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train,y_train)

model.score(X_test,y_test)

from sklearn.model_selection import cross_val_score
print(cross_val_score(GaussianNB(),X_train, y_train, cv=5))

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

pred = model.predict(X_train) 
accuracy_score(y_train, pred)

confusion_matrix(y_train, pred)

predicted_test = model.predict(X_test)
accuracy_score(y_test, predicted_test)

from sklearn.metrics import roc_curve, auc
import numpy as np

fpr, tpr, thresholds =roc_curve(y_test, predicted_test)
roc_auc = auc(fpr, tpr)
print("Area under the ROC curve : %f" % roc_auc)

i = np.arange(len(tpr)) # index for df
roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})
roc.iloc[(roc.tf-0).abs().argsort()[:1]]

# Plot tpr vs 1-fpr
fig, ax = plt.subplots()
plt.plot(roc['tpr'])
plt.plot(roc['1-fpr'], color = 'purple')
plt.xlabel('1-False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
ax.set_xticklabels([])

from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score

print(classification_report(y_test, predicted_test))

cma = confusion_matrix(y_test, predicted_test)

fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(cma, cmap=plt.cm.Blues, alpha=0.3)
for i in range(cma.shape[0]):
    for j in range(cma.shape[1]):
        ax.text(x=j, y=i,s=cma[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

"""# Random forest Classifier"""

clf_forest = rf(n_estimators=100, max_depth=10)
clf_forest.fit(X_train, y_train)

pred = clf_forest.predict(X_train)
accuracy_score(y_train, pred)

confusion_matrix(y_train, pred)

pred_test = clf_forest.predict(X_test)
accuracy_score(y_test, pred_test)

from sklearn.model_selection import GridSearchCV

parameters = {'n_estimators':[150,200,250,300], 'max_depth':[15,20,25]}
forest =rf()
clf = GridSearchCV(estimator=forest, param_grid=parameters, n_jobs=-1, cv=5)

clf.fit(input, target)

clf.best_params_

clf.best_score_

from sklearn.metrics import roc_curve, auc
import numpy as np

fpr, tpr, thresholds =roc_curve(y_test, pred_test)
roc_auc = auc(fpr, tpr)
print("Area under the ROC curve : %f" % roc_auc)

i = np.arange(len(tpr)) # index for df
roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})
roc.iloc[(roc.tf-0).abs().argsort()[:1]]

# Plot tpr vs 1-fpr
fig, ax = plt.subplots()
plt.plot(roc['tpr'])
plt.plot(roc['1-fpr'], color = 'red')
plt.xlabel('1-False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
ax.set_xticklabels([])

from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score

print(classification_report(y_test, pred_test))

cma = confusion_matrix(y_test, pred_test)

fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(cma, cmap ="coolwarm_r", alpha=0.3)
for i in range(cma.shape[0]):
    for j in range(cma.shape[1]):
        ax.text(x=j, y=i,s=cma[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()